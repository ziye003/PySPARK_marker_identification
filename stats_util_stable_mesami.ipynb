{"cells":[{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport scipy.stats\nfrom rpy2 import robjects as ro\nfrom rpy2.robjects import pandas2ri, Formula\n\n\nclass Association_spark(object):\n    def __init__(self, regression = 'logistic', family = None):\n        \"\"\"For performing large-scale regression-based association analysis in PySpark\n\n        Args:\n            regression  (str): type of regressions: [linear, logistic, ordinal logistic, lmer, glm]\n            family (str): family name of regression, only used with glm Regression (glm(formula, family=familytype(link=linkfunction), data=))\n                          Family\t        Default Link Function\n                          binomial\t        (link = \"logit\")\n                          gaussian\t        (link = \"identity\")\n                          Gamma\t            (link = \"inverse\")  \n                          inverse.gaussian\t(link = \"1/mu^2\")\n                          poisson\t        (link = \"log\")\n                          quasi\t            (link = \"identity\", variance = \"constant\")\n                          quasibinomial\t    (link = \"logit\")\n                          quasipoisson\t    (link = \"log\")\n  \n        Returns:\n            object: It returns an instance of Association_spark class.\n        \"\"\"   \n        \n        self.regression = regression\n        self.family = family\n        self.supported_regression_types = ['linear', 'logistic', 'ordinal logistic', 'glm', 'lmer', 'glmer']\n    \n\n    def find_association(self, sdf, dv, iv, groupby = 'metabolite', spark = spark, ordered_categories = ['1.0', '2.0', '3.0', '4.0', '5.0'], **args):\n        \"\"\"For performing large-scale regression-based association analysis in PySpark\n\n        Args:\n            sdf (spark dataframe): A spark data frame in long format\n            dv (str): Dependent variable in your regeression model\n            iv (str): Independent variable in your regression model\n            groupby (str, optional): The group name by which you want to partition. Defaults to \"metabolite\".\n            spark (str, optional): The instance of a spark engine. This argument is optional. Defaults to \"spark\".\n            ordered_categories ([str], optional): A list of ordinal labels. This argument is only required in ordinal logistic regression. Defaults to ['1.0', '2.0', '3.0', '4.0', '5.0'].\n\n        Returns:\n            spark dataframe: It returns the regression analysis result for each metabolite. \n        \"\"\"   \n        \n        covariate = args['covariate']\n        schema_de = StructType([ \\\n            StructField(groupby,StringType(),True), \\\n            StructField('Estimate',DoubleType(),True), \\\n            StructField('Std_Error', DoubleType(),False), \\\n            StructField('z_value',DoubleType(),False), \\\n            StructField('P', DoubleType(),False), \\\n            StructField('Support', IntegerType(),False), \\\n            StructField('N', IntegerType(),False), \\\n            StructField('Error', StringType(),False) \\\n            ])\n        \n        schema_lmer = StructType([ \\\n            StructField(groupby,StringType(),True), \\\n            StructField('Estimate',DoubleType(),True), \\\n            StructField('Std_Error', DoubleType(),False), \\\n            StructField('df',DoubleType(),False), \\\n            StructField('z_value',DoubleType(),False), \\\n            StructField('P', DoubleType(),False), \\\n            StructField('Support', IntegerType(),False), \\\n            StructField('N', IntegerType(),False), \\\n            StructField('Error', StringType(),False) \\\n            ])\n        \n        schema_glmer = StructType([ \\\n            StructField(groupby,StringType(),True), \\\n            StructField('Estimate',DoubleType(),True), \\\n            StructField('Std_Error', DoubleType(),False), \\\n            StructField('z_value',DoubleType(),False), \\\n            StructField('P', DoubleType(),False), \\\n            StructField('Support', IntegerType(),False), \\\n            StructField('N', IntegerType(),False), \\\n            StructField('Error', StringType(),False), \\\n            StructField('Warnings', StringType(),False) \\\n            ])\n        \n        schema_dict = {\n          'lmer': schema_lmer, \n          'glmer': schema_glmer\n        }\n        schema = schema_dict.get(self.regression, schema_de)\n#         schema = schema_lmer if self.regression == 'lmer' | self.regression == 'glmer' else schema_de\n       \n        regression_analysis = get_regression(self.regression, self.family, schema, dv, iv, groupby, covariate, ordered_categories)   \n        return sdf.groupby(groupby).apply(regression_analysis)\n\n\n\ndef get_regression(regression, family, schema, dv, iv, groupby, covariate, ordered_categories):\n\n  \n    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n    def glmeRegression_R_spark(key, df):\n        pandas2ri.activate()\n        R = ro.r\n        R('library(lme4)')\n        R('library(lmerTest)')\n        \n        \n        N = len(df)\n        df = df.dropna(subset = [dv.replace('`', '')]) \n        support = len(df)\n        \n        \n        covariate_list = [x for x in covariate.split('+') if x !='']\n        formula = Formula( ' + '.join([x for x in [f'{dv} ~ {iv}', f'{covariate}'] if x !='']))\n        error='no'\n        warnings='no'\n\n        try:\n            M = R.glmer(formula, data = df, family = R(family))\n        except Exception as e:\n            error += '\\\\' + str(e)\n            res = [np.nan] * 4\n            \n        try:\n            res = R.summary(M).rx2('coefficients')[1].tolist()\n            warning = R.summary(M).rx2('optinfo').rx2('conv').rx2('lme4').rx2('messages')\n#      \n            if warning:\n#               if len(warning)>1:\n#                 err = '_'.join([i for i in warning])\n# #               err='no'\n#               else:\n#               warnings = R.summary(M).rx2('optinfo').rx2('conv').rx2('lme4').rx2('messages')[1]\n#               print(warning)\n#               print(len(warning))\n# #               print(warning.shape)\n#               if len(warning)>1:\n# #                 err = '_'.join([i for i in warning])\n#                 err=warning[0]\n#               else:\n\n              err = 'warning'\n#               err=str(type(warning))\n              wtype=str(type(warning))\n              if wtype=='<class \\'rpy2.robjects.vectors.ListVector\\'>':\n\n                pandas2ri.activate()\n                R = ro.r\n                R('library(lme4)')\n                R('library(lmerTest)')\n                warning1 = R.summary(M).rx2('optinfo').rx2('conv').rx2('lme4').rx2('messages')[0]\n                warning2 = R.summary(M).rx2('optinfo').rx2('conv').rx2('lme4').rx2('messages')[1]\n#                 err = 'multiple list warnings'\n                err=warning1[0]+' ; ' +warning2[0]\n              if wtype=='<class \\'rpy2.robjects.vectors.StrVector\\'>':\n#                 err = warning[1]\n#                 pandas2ri.activate()\n#                 R = ro.r\n#                 R('library(lme4)')\n#                 R('library(lmerTest)')\n#                 warning = R.summary(M).rx2('optinfo').rx2('conv').rx2('lme4').rx2('messages')[1]\n                err = str(warning)\n#                 err = 'multiple string warnings'\n            else:\n              err='no'\n            warnings=err\n        except Exception as e:\n            error += '\\\\' +str(e)\n            \n            res = [np.nan] * 4\n  \n        pdf = pd.DataFrame([list(key) + res + [support, N, error,warnings]], columns = [groupby, 'Estimate', 'Std_Error', 'z_value', 'P', 'Support', 'N', 'Error','Warnings'])  \n        pdf.fillna(value = 9999, inplace = True)\n        return pdf  \n  \n    \n    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n    def lmeRegression_R_spark(key, df):\n        pandas2ri.activate()\n        R = ro.r\n        R('library(lme4)')\n        R('library(lmerTest)')\n        \n        \n        N = len(df)\n        df = df.dropna(subset = [dv.replace('`', '')]) \n        support = len(df)\n        \n        \n        covariate_list = [x for x in covariate.split('+') if x !='']\n        formula = Formula( ' + '.join([x for x in [f'{dv} ~ {iv}', f'{covariate}'] if x !='']))\n        error = 'no'\n        \n        try:\n            M = R.lmer(formula, data = df)\n        except Exception as e:\n            error = str(e)\n            res = [np.nan] * 5\n\n        try:\n            res = R.summary(M).rx2('coefficients')[1].tolist()\n        except Exception as e:\n            error = str(e)\n            res = [np.nan] * 6\n  \n        pdf = pd.DataFrame([list(key) + res + [support, N, error]], columns = [groupby, 'Estimate', 'Std_Error', 'df', 'z_value', 'P', 'Support', 'N', 'Error'])  \n        return pdf\n    \n  \n    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n    def glmRegression_R_spark(key, df):\n        pandas2ri.activate()\n        R = ro.r \n        N = len(df)\n        df = df.dropna(subset = [dv.replace('`', '')]) \n        support = len(df)\n        \n        covariate_list = [x for x in covariate.split('+') if x !='']\n        formula = Formula( ' + '.join([x for x in [f'{dv} ~ {iv}', f'{covariate}'] if x !='']))\n        error = 'no'\n        \n        try:\n            M = R.glm(formula, data=df, family = R(family))\n\n        except Exception as e:\n            error = str(e)\n            res = [np.nan] * 4\n        try:\n            R.summary(M).rx2('coefficients')[len(covariate_list) + 1]\n            res = R.summary(M).rx2('coefficients')[1].tolist()\n        except Exception as e:\n            error = str(e)\n            res = [np.nan] * 4\n\n        pdf = pd.DataFrame([list(key) + res + [support, N, error]], columns = [groupby, 'Estimate', 'Std_Error', 'z_value', 'P', 'Support', 'N', 'Error'])\n        pdf.fillna(value = 9999, inplace = True)\n        return pdf\n    \n\n    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n    def linearRegression_R_spark(key, df):\n        pandas2ri.activate()\n        R = ro.r \n        N = len(df)\n#         dv = dv.split('(')[1].split(')')[0]\n#         df = df.dropna(subset = [dv.split('(')[1].split(')')[0].replace('`', '')]) \n        df = df.dropna(subset = [dv.replace('`', '')])\n        support = len(df) \n\n        covariate_list = [x for x in covariate.split('+') if x !='']\n        formula = Formula( ' + '.join([x for x in [f'{dv} ~ {iv}', f'{covariate}'] if x !='']))\n        error = 'no'\n\n        try:\n            M = R.lm(formula, data=df)\n\n        except Exception as e:\n            error = str(e)\n            res = [np.nan] * 4\n        try:\n            R.summary(M).rx2('coefficients')[len(covariate_list) + 1]\n            res = R.summary(M).rx2('coefficients')[1].tolist()\n        except Exception as e:\n            error = str(e)\n            res = [np.nan] * 4\n\n        pdf = pd.DataFrame([list(key) + res + [support, N, error]], columns = [groupby, 'Estimate', 'Std_Error', 'z_value', 'P', 'Support', 'N', 'Error'])\n        pdf.fillna(value = 9999, inplace = True)\n        return pdf\n\n\n    \n    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n    def ordinalLogisticRegression_R_spark(key, df):  \n        pandas2ri.activate()\n        R = ro.r  \n        R('library(MASS)')\n\n        \n        N = len(df)\n        df = df.dropna(subset = [dv.replace('`', '')]) \n        support = len(df)\n\n\n        df[dv] = df[dv].astype('string')\n        df[dv] = pd.Categorical(\n            df[dv],\n            categories = ordered_categories\n        )\n\n        covariate_list = [x for x in covariate.split('+') if x !='']\n        formula = Formula( ' + '.join([x for x in [f'{dv} ~ {iv}', f'{covariate}'] if x !='']))\n        error = 'no'\n\n        try:\n            M = R.polr(formula, data=df, Hess = True)\n        except Exception as e:\n            error = str(e)\n            res = [np.nan] * 4\n            \n        try:\n            R.summary(M).rx2('coefficients')[len(covariate_list) + 1]\n            res = R.summary(M).rx2('coefficients')[0].tolist()\n            res.append(scipy.stats.t.sf(np.abs(res[2]), df.shape[0] - 1)*2)\n        except Exception as e:\n            error += '\\\\' + str(e)\n            res = [np.nan] * 4\n\n        pdf = pd.DataFrame([list(key) + res + [support, N, error]], columns = [groupby, 'Estimate', 'Std_Error', 'z_value', 'P', 'Support', 'N', 'Error'])\n        pdf.fillna(value = 999, inplace = True)\n        return pdf\n      \n\n    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n    def logisticRegression_R_spark(key, df):\n        pandas2ri.activate()\n        R = ro.r \n        N = len(df)\n        df = df.dropna(subset = [dv.replace('`', '')]) \n        support = len(df)\n\n        covariate_list = [x for x in covariate.split('+') if x !='']\n        group_key = df['metabolite'].iloc[0]\n        formula = Formula( ' + '.join([x for x in [f'{dv} ~ {iv}', f'{covariate}'] if x !='']))\n        error = 'no'\n        \n        try:\n            M = R.glm(formula, data=df, family = \"binomial\")\n\n        except Exception as e:\n            error = str(e)\n            res = [np.nan] * 4\n        try:\n            R.summary(M).rx2('coefficients')[len(covariate_list) + 1]\n            res = R.summary(M).rx2('coefficients')[1].tolist()\n        except Exception as e:\n            error = str(e)\n            res = [np.nan] * 4\n\n        pdf = pd.DataFrame([list(key) + res + [support, N, error]], columns = [groupby, 'Estimate', 'Std_Error', 'z_value', 'P', 'Support', 'N', 'Error'])\n#         pdf.fillna(value = 9999, inplace = True)\n        return pdf\n\n    if regression == 'ordinal logistic':\n        return ordinalLogisticRegression_R_spark\n    elif regression == 'logistic':\n        return logisticRegression_R_spark \n    elif regression == 'linear':\n        return linearRegression_R_spark\n    elif regression == 'glm':\n        return glmRegression_R_spark\n    elif regression == 'lmer':\n        return lmeRegression_R_spark\n    elif regression == 'glmer':\n        return glmeRegression_R_spark\n    else:\n        return 'Not yet implemented!'\n      \n\n\ndef add_confint(df, alpha =0.05, n_var = 4):\n    degree_freedom = df['Support'] - 1 - n_var\n    tscore = scipy.stats.t.isf(alpha / 2, degree_freedom)\n    margin_error = tscore * df['Std_Error']\n    df['Estimate_lower_bound'] = df['Estimate'] - margin_error\n    df['Estimate_upper_bound'] = df['Estimate'] + margin_error\n    return df.loc[:, ['mtb', 'Estimate', 'Estimate_lower_bound', 'Estimate_upper_bound', 'Std_Error', 'z_value', 'P', 'Support', 'N']]\n  \n  \n# def add_confint(df, alpha =0.05, n_var = 4):\n#     degree_freedom = df['Support'] - 1 - n_var\n#     tscore = scipy.stats.t.isf(alpha / 2, degree_freedom)\n#     margin_error = tscore * df['Std. Error']\n#     df['Estimate_lower_bound'] = df['Estimate'] - margin_error\n#     df['Estimate_upper_bound'] = df['Estimate'] + margin_error\n#     return df.loc[:, ['mtb', 'Estimate', 'Estimate_lower_bound', 'Estimate_upper_bound', 'Std. Error', 'z value', 'Pr(>|z|)', 'Support', 'N']]\n    \ndef add_confint(df, alpha =0.05, n_var = 4):\n    degree_freedom = df['Support'] - 1 - n_var\n    tscore = scipy.stats.t.isf(alpha / 2, degree_freedom)\n    margin_error = tscore * df['Std_Error']\n    df['Estimate_lower_bound'] = df['Estimate'] - margin_error\n    df['Estimate_upper_bound'] = df['Estimate'] + margin_error\n    df['margin_error'] =  margin_error\n    return df.loc[:, ['metabolite', 'Estimate', 'Estimate_lower_bound', 'Estimate_upper_bound', 'margin_error','Std_Error', 'z_value', 'P', 'Support', 'N']]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d3273a58-afaa-467a-a0a2-25db72bb2ce7"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"stats_util_stable_mesami","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3013655503687904}},"nbformat":4,"nbformat_minor":0}
